# CoverageEval Experiments

## Experiment Logs

#### Zero Shot Experimentation
| Model                         | Match | Stmt | Branch | 
|-------------------------------|-------|-------|-------|
| OpenAI GPT-4 (gpt-4           | 25.75 | 84.47 | 20.16 |
| OpenAI GPT-3.5 (gpt-3.5-turbo)| 0     | 39.87 | 8.33  |
| Google BARD (text-bison-001)  |  0    | 81.27 | 17.21 |
| Anthropic Claude (claude-1.3  | 3.9   | 84.47 | 20.07 |

#### One Shot Experimentation
| Model                         | Match | Stmt | Branch | 
|-------------------------------|-------|-------|-------|
| OpenAI GPT-4 (gpt-4           | 22.85 | 90.71 | 22.65 |
| OpenAI GPT-3.5 (gpt-3.5-turbo)|  8.17 | 76.53 | 17.17 |
| Google BARD (text-bison-001)  |  1.87 | 86.93 | 19.63 |
| Anthropic Claude (claude-1.3  |  4.83 | 83.21 | 19.16 |

#### Multi Shot Experimentation
| Model                         | Match | Stmt | Branch | 
|-------------------------------|-------|-------|-------|
| OpenAI GPT-4 (gpt-4           | 30.04 | 90.5  | 22.5  |
| OpenAI GPT-3.5 (gpt-3.5-turbo)| 11.03 | 82.29 | 17.9  |
| Google BARD (text-bison-001)  | 21.56 | 85.66 | 20.52 |
| Anthropic Claude (claude-1.3  | 6.88  | 55.7  | 12.23 |
## Sample Prompts

## Model Inputs and Outputs
